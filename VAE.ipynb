{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "no_of_sample = 10\n",
    "CUDA = True\n",
    "BATCH_SIZE = 100\n",
    "LOG_INTERVAL = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CelebaDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, im_name_list, resize_dim, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.im_list = im_name_list\n",
    "        self.resize_dim = resize_dim\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = Image.open(os.path.join(self.root_dir, self.im_list[idx]))\n",
    "        im = np.array(im)\n",
    "        im = imresize(im, self.resize_dim, interp='nearest')\n",
    "        im = im / 255\n",
    "\n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "\n",
    "        return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors. numpy image: H x W x C, torch image: C X H X W\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, image, invert_arrays=True):\n",
    "\n",
    "        if invert_arrays:\n",
    "            image = image.transpose((2, 0, 1))\n",
    "\n",
    "        return torch.from_numpy(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conv_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride, pool_kernel_size=(2, 2)):\n",
    "        super(Conv_Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding, stride)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding, stride)\n",
    "        self.pool = nn.MaxPool2d(pool_kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.block1 = Conv_Block(3, 64, (3, 3), 1, 1)  # 64\n",
    "        self.block2 = Conv_Block(64, 128, (3, 3), 1, 1)  # 32\n",
    "        self.block3 = Conv_Block(128, 256, (3, 3), 1, 1)  # 16\n",
    "        self.block4 = Conv_Block(256, 32, (3, 3), 1, 1)  # 8\n",
    "\n",
    "        # Decoder\n",
    "        self.fct_decode = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, (3, 3), padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 16\n",
    "            nn.Conv2d(64, 64, (3, 3), padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 32\n",
    "            nn.Conv2d(64, 64, (3, 3), padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 64\n",
    "            nn.Conv2d(64, 16, (3, 3), padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 128\n",
    "        )\n",
    "\n",
    "        self.final_decod_mean = nn.Conv2d(16, 3, (3, 3), padding=1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        '''return mu_z and logvar_z'''\n",
    "\n",
    "        x = F.elu(self.block1(x))\n",
    "        x = F.elu(self.block2(x))\n",
    "        x = F.elu(self.block3(x))\n",
    "        x = F.elu(self.block4(x))\n",
    "\n",
    "        return x[:, :16, :, :], x[:, 16:, :, :]  # output shape - batch_size x 16 x 8 x 8\n",
    "\n",
    "    def reparameterize(self, mu: Variable, logvar: Variable) -> Variable:\n",
    "\n",
    "        if self.training:\n",
    "            # multiply log variance with 0.5, then in-place exponent\n",
    "            # yielding the standard deviation\n",
    "\n",
    "            sample_z = []\n",
    "            for _ in range(no_of_sample):\n",
    "                std = logvar.mul(0.5).exp_()  # type: Variable\n",
    "                eps = Variable(std.data.new(std.size()).normal_())\n",
    "                sample_z.append(eps.mul(std).add_(mu))\n",
    "\n",
    "            return sample_z\n",
    "\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "\n",
    "        z = self.fct_decode(z)\n",
    "        z = self.final_decod_mean(z)\n",
    "        z = F.sigmoid(z)\n",
    "\n",
    "        return z.view(-1, 3 * 128 * 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        if self.training:\n",
    "            return [self.decode(z) for z in z], mu, logvar\n",
    "        else:\n",
    "            return self.decode(z), mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar) -> Variable:\n",
    "        # how well do input x and output recon_x agree?\n",
    "\n",
    "        if self.training:\n",
    "            BCE = 0\n",
    "            for recon_x_one in recon_x:\n",
    "                BCE += F.binary_cross_entropy(recon_x_one, x.view(-1, 3 * 128 * 128))\n",
    "            BCE /= len(recon_x)\n",
    "        else:\n",
    "            BCE = F.binary_cross_entropy(recon_x, x.view(-1, 3 * 128 * 128))\n",
    "\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        KLD /= BATCH_SIZE * 3 * 128 * 128\n",
    "\n",
    "        return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (block1): Conv_Block(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Conv_Block(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block3): Conv_Block(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block4): Conv_Block(\n",
       "    (conv1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fct_decode): Sequential(\n",
       "    (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ELU(alpha=1.0)\n",
       "    (8): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (9): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ELU(alpha=1.0)\n",
       "    (11): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  )\n",
       "  (final_decod_mean): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=VAE()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, train_loader):\n",
    "    # toggle model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # in the case of MNIST, len(train_loader.dataset) is 60000\n",
    "    # each `data` is of BATCH_SIZE samples and has shape [128, 1, 28, 28]\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = Variable(data.type(torch.FloatTensor))\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # push whole batch of data through VAE.forward() to get recon_loss\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        # calculate scalar loss\n",
    "        loss = model.loss_function(recon_batch, data, mu, logvar)\n",
    "        # calculate the gradient of the loss w.r.t. the graph leaves\n",
    "        # i.e. input variables -- by the power of pytorch!\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data),\n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader),\n",
    "                                                                           loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    # each data is of BATCH_SIZE (default 128) samples\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = Variable(data.type(torch.FloatTensor), volatile=True)\n",
    "        if CUDA:\n",
    "            # make sure this lives on the GPU\n",
    "            data = data.cuda()\n",
    "\n",
    "        # we're only going to infer, so no autograd at all required: volatile=True\n",
    "\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += model.loss_function(recon_batch, data, mu, logvar).item()\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            # for the first 128 batch of the epoch, show the first 8 input digits\n",
    "            # with right below them the reconstructed output digits\n",
    "            comparison = torch.cat([data[:n],\n",
    "                                    recon_batch.view(BATCH_SIZE, 3, 128, 128)[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                       './celeba/comparison/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "        # break #To save time\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.006945\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.005701\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.005313\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.005326\n",
      "====> Epoch: 1 Average loss: 0.0055\n",
      "====> Test set loss: 0.0052\n",
      "Train Epoch: 2 [0/20000 (0%)]\tLoss: 0.005308\n",
      "Train Epoch: 2 [5000/20000 (25%)]\tLoss: 0.005157\n",
      "Train Epoch: 2 [10000/20000 (50%)]\tLoss: 0.005260\n",
      "Train Epoch: 2 [15000/20000 (75%)]\tLoss: 0.005275\n",
      "====> Epoch: 2 Average loss: 0.0052\n",
      "====> Test set loss: 0.0051\n",
      "Train Epoch: 3 [0/20000 (0%)]\tLoss: 0.005259\n",
      "Train Epoch: 3 [5000/20000 (25%)]\tLoss: 0.005146\n",
      "Train Epoch: 3 [10000/20000 (50%)]\tLoss: 0.005237\n",
      "Train Epoch: 3 [15000/20000 (75%)]\tLoss: 0.005171\n",
      "====> Epoch: 3 Average loss: 0.0052\n",
      "====> Test set loss: 0.0050\n",
      "Train Epoch: 4 [0/20000 (0%)]\tLoss: 0.005041\n",
      "Train Epoch: 4 [5000/20000 (25%)]\tLoss: 0.005215\n",
      "Train Epoch: 4 [10000/20000 (50%)]\tLoss: 0.005172\n",
      "Train Epoch: 4 [15000/20000 (75%)]\tLoss: 0.005237\n",
      "====> Epoch: 4 Average loss: 0.0052\n",
      "====> Test set loss: 0.0050\n",
      "Train Epoch: 5 [0/20000 (0%)]\tLoss: 0.005092\n",
      "Train Epoch: 5 [5000/20000 (25%)]\tLoss: 0.005204\n",
      "Train Epoch: 5 [10000/20000 (50%)]\tLoss: 0.005191\n",
      "Train Epoch: 5 [15000/20000 (75%)]\tLoss: 0.005151\n",
      "====> Epoch: 5 Average loss: 0.0051\n",
      "====> Test set loss: 0.0050\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    root_dir = \"./celeba/img_align_celeba\"\n",
    "    image_files = os.listdir(root_dir)\n",
    "    train_dataset = CelebaDataset(root_dir, image_files[:20000], (128, 128), transforms.Compose([ToTensor()]))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=10, shuffle=True)\n",
    "\n",
    "    #Take only 1000 images in test\n",
    "    test_dataset = CelebaDataset(root_dir, image_files[20000:21000], (128, 128), transforms.Compose([ToTensor()]))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=10, shuffle=True)\n",
    "\n",
    "    EPOCHS = 5\n",
    "    model = VAE()\n",
    "    model= model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(epoch, model, optimizer, train_loader)\n",
    "        test(epoch, model, test_loader)\n",
    "\n",
    "        # 64 sets of random ZDIMS-float vectors, i.e. 64 locations / MNIST\n",
    "        # digits in latent space\n",
    "        sample = Variable(torch.randn(16, 16, 8, 8))\n",
    "        if CUDA:\n",
    "            sample = sample.cuda()\n",
    "        sample1 = model.decode(sample).cpu()\n",
    "\n",
    "        # save out as an 8x8 matrix of MNIST digits\n",
    "        # this will give you a visual idea of how well latent space can generate things\n",
    "        # that look like digits\n",
    "        save_image(sample1.data.view(16, 3, 128, 128), './celeba/sample/reconstruction' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
